<!DOCTYPE html>
<html lang="en-GB">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Cut GPU from machine learning &#x2d; Thomas Metcalfe</title>
<meta name="robots" content="max-image-preview:large">

<!-- The SEO Framework by Sybre Waaijer -->
<meta name="robots" content="max-snippet:-1,max-image-preview:standard,max-video-preview:-1">
<meta name="description" content="Thomas Metcalfe details advances in machine learning engineering research cutting graphics processing (GPUs) from machine learning model training and inference">
<meta property="og:image" content="wp-content/uploads/2021/05/5337dfa7-gpu-blog-post-scaled.jpg">
<meta property="og:image:width" content="2560">
<meta property="og:image:height" content="1344">
<meta property="og:locale" content="en_GB">
<meta property="og:type" content="article">
<meta property="og:title" content="Cut GPU from machine learning">
<meta property="og:description" content="Thomas Metcalfe details advances in machine learning engineering research cutting graphics processing (GPUs) from machine learning model training and inference">
<meta property="og:url" content="/cut-gpu-from-machine-learning/">
<meta property="og:site_name" content="Thomas Metcalfe">
<meta property="article:published_time" content="2021-05-19T08:23+00:00">
<meta property="article:modified_time" content="2022-04-09T11:00+00:00">
<meta property="og:updated_time" content="2022-04-09T11:00+00:00">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Cut GPU from machine learning">
<meta name="twitter:description" content="Thomas Metcalfe details advances in machine learning engineering research cutting graphics processing (GPUs) from machine learning model training and inference">
<meta name="twitter:image" content="wp-content/uploads/2021/05/5337dfa7-gpu-blog-post-scaled.jpg">
<meta name="twitter:image:width" content="2560">
<meta name="twitter:image:height" content="1344">
<link rel="canonical" href="/cut-gpu-from-machine-learning/">
<script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"/","name":"Thomas Metcalfe"}},{"@type":"ListItem","position":2,"item":{"@id":"/category/machine-learning/","name":"Machine learning"}},{"@type":"ListItem","position":3,"item":{"@id":"/cut-gpu-from-machine-learning/","name":"Cut GPU from machine learning"}}]}</script>
<!-- / The SEO Framework by Sybre Waaijer | 215.65ms meta | 18.41ms boot -->

<link rel="dns-prefetch" href="//s.w.org">
<link rel="alternate" type="application/rss+xml" title="Thomas Metcalfe &raquo; Feed" href="/feed/">
<link rel="alternate" type="application/rss+xml" title="Thomas Metcalfe &raquo; Comments Feed" href="/comments/feed/">
		<script>
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.1\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/13.0.1\/svg\/","svgExt":".svg","source":{"concatemoji":"\/wp-includes\/js\/wp-emoji-release.min.js?ver=5.7.1"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([55357,56424,8205,55356,57212],[55357,56424,8203,55356,57212])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style>img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}</style>
	<link rel="stylesheet" id="wp-block-library-css" href="/wp-includes/css/dist/block-library/style.min.css?ver=5.7.1" media="all">
<link rel="stylesheet" id="wp-block-library-theme-css" href="/wp-includes/css/dist/block-library/theme.min.css?ver=5.7.1" media="all">
<link rel="stylesheet" id="twenty-twenty-one-style-css" href="/wp-content/themes/twentytwentyone/style.css?ver=1.3" media="all">
<style id="twenty-twenty-one-style-inline-css">:root{--global--color-background: #391f3a;--global--color-primary: #fff;--global--color-secondary: #fff;--button--color-background: #fff;--button--color-text-hover: #fff;--table--stripes-border-color: rgba(240, 240, 240, 0.15);--table--stripes-background-color: rgba(240, 240, 240, 0.15);}</style>
<link rel="stylesheet" id="twenty-twenty-one-print-style-css" href="/wp-content/themes/twentytwentyone/assets/css/print.css?ver=1.3" media="print">
<link rel="https://api.w.org/" href="/wp-json/">
<link rel="alternate" type="application/json" href="/wp-json/wp/v2/posts/59">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="/wp-includes/wlwmanifest.xml"> 
<link rel="alternate" type="application/json+oembed" href="/wp-json/oembed/1.0/embed?url=%2Fcut-gpu-from-machine-learning%2F">
<link rel="alternate" type="text/xml+oembed" href="/wp-json/oembed/1.0/embed?url=%2Fcut-gpu-from-machine-learning%2F&#038;format=xml">
<style id="custom-background-css">body.custom-background { background-color: #391f3a; }</style>
	</head>

<body class="post-template-default single single-post postid-59 single-format-standard custom-background wp-embed-responsive is-dark-theme no-js singular has-main-navigation">
<div id="page" class="site">
	<a class="skip-link screen-reader-text" href="#content">Skip to content</a>

	
<header id="masthead" class="site-header has-title-and-tagline has-menu" role="banner">

	

<div class="site-branding">

	
						<p class="site-title"><a href="/">Thomas Metcalfe</a></p>
			
			<p class="site-description">
			Machine learning and people learning		</p>
	</div>
<!-- .site-branding -->
	
	<nav id="site-navigation" class="primary-navigation" role="navigation" aria-label="Primary menu">
		<div class="menu-button-container">
			<button id="primary-mobile-menu" class="button" aria-controls="primary-menu-list" aria-expanded="false">
				<span class="dropdown-icon open">Menu					<svg class="svg-icon" width="24" height="24" aria-hidden="true" role="img" focusable="false" viewbox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.5 6H19.5V7.5H4.5V6ZM4.5 12H19.5V13.5H4.5V12ZM19.5 18H4.5V19.5H19.5V18Z" fill="currentColor"></path></svg>				</span>
				<span class="dropdown-icon close">Close					<svg class="svg-icon" width="24" height="24" aria-hidden="true" role="img" focusable="false" viewbox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M12 10.9394L5.53033 4.46973L4.46967 5.53039L10.9393 12.0001L4.46967 18.4697L5.53033 19.5304L12 13.0607L18.4697 19.5304L19.5303 18.4697L13.0607 12.0001L19.5303 5.53039L18.4697 4.46973L12 10.9394Z" fill="currentColor"></path></svg>				</span>
			</button><!-- #primary-mobile-menu -->
		</div>
<!-- .menu-button-container -->
		<div class="primary-menu-container"><ul id="primary-menu-list" class="menu-wrapper">
<li id="menu-item-14" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-14"><a href="/">Home</a></li>
<li id="menu-item-15" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-15"><a href="/about/">About</a></li>
</ul></div>	</nav><!-- #site-navigation -->

</header><!-- #masthead -->

	<div id="content" class="site-content">
		<div id="primary" class="content-area">
			<main id="main" class="site-main" role="main">

<article id="post-59" class="post-59 post type-post status-publish format-standard has-post-thumbnail hentry category-machine-learning tag-deployment tag-gpu tag-machine-learning tag-optimization tag-research tag-training entry">

	<header class="entry-header alignwide">
		<h1 class="entry-title">Cut GPU from machine learning</h1>		
		
			<figure class="post-thumbnail">
				<img width="1568" height="823" src="/wp-content/uploads/2021/05/5337dfa7-gpu-blog-post-1568x823.jpg" class="attachment-post-thumbnail size-post-thumbnail wp-post-image wp-stateless-item" alt="" srcset="/wp-content/uploads/2021/05/5337dfa7-gpu-blog-post-1568x823.jpg 1568w, /wp-content/uploads/2021/05/5337dfa7-gpu-blog-post-300x158.jpg 300w, /wp-content/uploads/2021/05/5337dfa7-gpu-blog-post-1024x538.jpg 1024w, /wp-content/uploads/2021/05/5337dfa7-gpu-blog-post-768x403.jpg 768w, /wp-content/uploads/2021/05/5337dfa7-gpu-blog-post-1536x806.jpg 1536w, /wp-content/uploads/2021/05/5337dfa7-gpu-blog-post-2048x1075.jpg 2048w" sizes="(max-width: 1568px) 100vw, 1568px" style="width:100%;height:52.5%;max-width:2560px;" data-image-size="post-thumbnail" data-stateless-media-bucket="thomasmetcalfe.appspot.com" data-stateless-media-name="2021/05/5337dfa7-gpu-blog-post-scaled.jpg">							</figure><!-- .post-thumbnail -->

					</header><!-- .entry-header -->

	<div class="entry-content">
		
<p>This article reviews recent advances in machine learning that have provided <strong>competitive alternatives to graphics processing</strong>.&nbsp; To keep model operation timely, both business and research necessitate GPU access.&nbsp; Unfortunately, this hardware currently <strong>costs 10x the price per GB of memory</strong> compared to system RAM.</p>



<p>To cut our GPU dependency we can employ the following techniques:</p>



<h4>Training</h4>



<ul>
<li>Fine-tuning pre-trained models</li>
<li>Reducing computational complexity</li>
<li>CPU offloading</li>
</ul>



<h4>Inference</h4>



<ul>
<li>Optimising models for their deployment targets</li>
<li>Reducing models</li>
</ul>



<p>This post describes how these techniques improve performance and reviews their effectiveness.&nbsp; Cutting GPU allows more avenues for model deployment and democratizes model operation, so it’s promising that these techniques will continue to evolve.</p>



<h2>GPUs: how and why?</h2>



<p>Competition in ML research drove accuracy upwards by increasing the number of model parameters into the billions, and <strong>employing such magnitudes on quick timescales requires stream processing architectures</strong> <strong>like GPUs</strong>.&nbsp; Without this speedup, research labs can’t tune and repeat their experiments, and businesses are slower to meet consumer demand.</p>



<p>GPUs excel because they are designed for vector computations, which are very common in modern machine learning.&nbsp; Indeed, these computations can perform up to <strong>200x quicker on a GPU </strong><sup>[1]</sup>.&nbsp; We can understand the computations composing a machine learning model by considering training as a directed cyclic graph:</p>



<figure class="wp-block-image is-style-default"><img src="https://lh5.googleusercontent.com/pIZO9bGPQ9huzlqkgUjEAkDYONhBSW8cOqWibLf0pxW7TQbIg9f8byMff5THg37FPFc4HlOCjWOIofaFwOM_353cQFzRpCHoby_q8kgQl1R0fdHFBervAok11FyGp9fcL3VEz9vU" alt=""></figure>



<p>Here, golden circular nodes represent <strong>states </strong>which occupy GPU memory: batch, layer activations, gradients and model parameters.&nbsp; Grey rectangles represent <strong>vector</strong> <strong>computations</strong> from the connected states, and the result is stored in the state at the end of the outgoing edge<strong>.&nbsp; </strong>Going once clockwise around the graph represents <strong>one training loop,</strong> which repeats until convergence.&nbsp; Now let’s compare training with inference:</p>



<figure class="wp-block-image size-large is-resized"><img loading="lazy" src="/wp-content/uploads/2021/05/5b7b08bc-inferrence-graphcrop-1024x330.jpg" alt="" class="wp-image-68" width="610" height="196" srcset="/wp-content/uploads/2021/05/5b7b08bc-inferrence-graphcrop-1024x330.jpg 1024w, /wp-content/uploads/2021/05/5b7b08bc-inferrence-graphcrop-300x97.jpg 300w, /wp-content/uploads/2021/05/5b7b08bc-inferrence-graphcrop-768x248.jpg 768w, /wp-content/uploads/2021/05/5b7b08bc-inferrence-graphcrop-1536x495.jpg 1536w, /wp-content/uploads/2021/05/5b7b08bc-inferrence-graphcrop-2048x660.jpg 2048w, /wp-content/uploads/2021/05/5b7b08bc-inferrence-graphcrop-1568x506.jpg 1568w" sizes="(max-width: 610px) 100vw, 610px"></figure>



<p>Observe that <strong>much less GPU is required for inference</strong>: we hold just the batch and parameters in memory, and compute only the final layer activation with the forward pass.</p>



<h2>Cut GPU from model training</h2>



<h3>I: Fine-tuning pre-trained models</h3>



<p>Fine-tuning <strong>expedites our model training </strong>through better initialization, and gives the option to <strong>constrict additional training computations to just the final layers </strong>by using the inference of a pre-trained model as features.&nbsp; Re-using our pre-trained parameters this way is very cost-effective: we can train a large model on a huge domain-invariant dataset once and use it multiple times over for high generalization.</p>



<p>In many domains, research authors have open-sourced large generalized models and successfully fine-tuned them on varying downstream tasks.&nbsp; Some examples include:</p>



<ul>
<li>BERT, a language model trained on Wikipedia and book corpora which reached state-of-the-art on 11 NLP tasks <sup>[2]</sup>
</li>
<li>VGG16, an image classifier achieving 92.7% top-5 test accuracy on ImageNet <sup>[3]</sup>
</li>
</ul>



<p>This technique is <strong>extremely common due to its effectiveness, </strong>and libraries like transformers have made integration and fine-tuning simple by providing a universal interface<sup> [4]</sup>.</p>



<h3>II: Reducing computational complexity</h3>



<p>We can optimize bottleneck processes to run with reduced space and time complexity.&nbsp; In doing this, <strong>we increase GPU memory and time efficiency</strong> during operation.</p>



<p>One such process is <em>self-attention</em>, the mechanism of a transformer model, which has a space complexity of  <strong><em>O(L<sup>2</sup>)</em></strong> where <strong><em>L</em></strong> is the <em>input sequence length</em>.&nbsp; Over the past two years, a number of papers questioned whether this quadratic complexity is essential, offering solutions including:</p>



<ul>
<li>Locality sensitive hashing attention (e.g. ReFormer, SLIDE) <sup>[5, 6]</sup>
</li>
<li>Global attention and sliding window attention (e.g. LongFormer, BigBird)<sup> [7, 8]</sup>
</li>
<li>Linear attention (e.g. LinFormer)<sup> [9]</sup>
</li>
<li>Fast attention via positive orthogonal random features (e.g. Performer)<sup> [10]</sup>
</li>
</ul>



<h3>III: CPU offloading</h3>



<p>Moving computations to CPU<strong> reduces the GPU memory overhead by storing states in system RAM</strong>.&nbsp; We sacrifice a bit of speed to achieve this, but low-complexity computations can be offloaded to CPU without significant slowdown.</p>



<p>ZeRO-Offload executes this offload after the backward pass, moving gradients into RAM and performing parameter updates on CPU <sup>[11]</sup>.&nbsp; This strategy minimizes communication between CPU and GPU, reserving<strong> 10x more GPU memory</strong>.</p>



<h2>Cut GPU from model inference</h2>



<h3>I: Optimising models for their deployment targets</h3>



<p>Once the model is trained, <strong>we accelerate the inference graph</strong> for the hardware on which we plan to deploy.&nbsp; We make the model smaller, faster, and highly optimized for environments that even don’t have <strong>any</strong> GPU access.</p>



<p>ONNX and ONNX Runtime are open source libraries to convert and run models at <strong>up to 17x their original speeds<sup> </sup></strong><sup>[12]</sup>.&nbsp; ONNX converts the forward pass into a smaller, interoperable format that is ready for deployment practically anywhere. The runtime then provides accelerators that take advantage of the specific compute capabilities of the target hardware.</p>



<h3>II: Reducing models</h3>



<p>We can shrink our trained model’s memory footprint by <strong>decreasing parameter precision, distilling it or even removing sets of parameters entirely (“pruning”)</strong>.&nbsp; These techniques sacrifice accuracy to reduce the size of the model while retaining its decision boundaries.</p>



<p>The benefits of reducing <strong>also carry over to inference speed.</strong>&nbsp; In the case of decreasing parameter precision, swapping to 8-bit integer parameters decreases model size by 75% <strong>and</strong> <strong>grants us a 4x speed boost</strong> because of faster integer arithmetic <sup>[13]</sup>.</p>



<h2>Review &amp; speculation</h2>



<p>Because inference requires no backward passes or parameter updates, the above techniques work extremely well to make <strong>pure CPU predictions viable</strong>.&nbsp; ONNX and ONNX Runtime have been very welcome additions to the machine learning engineering toolkit, and in some cases make inference <strong>even faster than GPU</strong>.&nbsp; The consequences are substantial: <strong>even model deployment to less powerful devices like mobile phones are possible</strong>.</p>



<p>Note that large tech companies like Microsoft, Google and IBM support much of the research that published the techniques discussed.&nbsp; It’s possible they have a vested interest in reducing market demand for graphics processors, which they currently don’t manufacture themselves.</p>



<p>In total, the trend of reducing GPU dependency does great things for research democracy, since it facilitates smaller laboratories reproducing state-of-the-art results from big-budget experiments.</p>



<p>Because of these reasons, <strong>we will continue to see more evolution in GPU cutting.</strong></p>



<h2>Conclusion</h2>



<p>GPU usage provides huge speed benefits to machine learning practitioners, with large costs that can be mitigated.&nbsp; The benefit of GPU architecture in model operation is making the following vector computations quicker:</p>



<ul>
<li>Forward pass</li>
<li>Backward pass</li>
<li>Parameter update</li>
</ul>



<p>The amount of GPU needed for training can be cut by <strong>fine-tuning, reducing computational complexity and offloading to CPU</strong>, but sacrifices in speed and accuracy must undergo consideration.&nbsp; Since inference only contains forward pass, <strong>optimizing and reducing models</strong> make prediction fast enough to cut GPUs entirely from production scenarios.&nbsp;&nbsp;</p>



<p>The advancements enabling this change brought dramatic cost reductions for both research and business, and opened <strong>new avenues for model deployment</strong>.&nbsp; This makes me very excited to see what comes next.</p>



<h2>References</h2>



<p>[1] <a href="https://drive.google.com/file/d/0B-crIDDaxB7_MjVlYTE2ODItNWRmMi00ZmMzLTk3MGQtMjBhNmRjNmZmMmY3/view">GPU Performance Analysis</a></p>



<p>[2] <a href="https://github.com/google-research/bert">BERT on Github, with links to original paper</a></p>



<p>[3] <a href="https://neurohive.io/en/popular-networks/vgg16/">Overview of VGG16</a></p>



<p>[4] <a href="https://github.com/huggingface/transformers/">🤗 transformers on Github, providing access to a vast library of state-of-the-art language models</a></p>



<p>[5] <a href="https://arxiv.org/abs/2001.04451">ReFormer: The efficient Transformer</a></p>



<p>[6] <a href="https://www.cs.rice.edu/~as143/Papers/SLIDE_MLSys.pdf">SLIDE: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems</a></p>



<p>[7] <a href="https://arxiv.org/abs/2004.05150">LongFormer: The long document Transformer</a></p>



<p>[8] <a href="https://arxiv.org/abs/2007.14062">BigBird: Transformers for longer sequences</a></p>



<p>[9] <a href="https://arxiv.org/abs/2006.04768">Linformer: Self-attention with linear complexity</a></p>



<p>[10] <a href="https://arxiv.org/abs/2009.14794">Rethinking attention with Performers</a></p>



<p>[11] <a href="https://arxiv.org/abs/2101.06840">ZeRO-Offload: Democratizing billion-scale model training</a></p>



<p>[12] <a href="https://onnx.ai/">ONNX: Open Neural Network Exchange</a></p>



<p>[13] <a href="https://arxiv.org/abs/2101.01321">I-BERT: Integer-only BERT quantization</a></p>
	</div>
<!-- .entry-content -->

	<footer class="entry-footer default-max-width">
		<div class="posted-by">
<span class="posted-on">Published <time class="entry-date published updated" datetime="2021-05-19T09:23:25+01:00">19 May 2021</time></span><span class="byline">By <a href="/author/thomas/" rel="author">thomas</a></span>
</div>
<div class="post-taxonomies">
<span class="cat-links">Categorised as <a href="/category/machine-learning/" rel="category tag">Machine learning</a> </span><span class="tags-links">Tagged <a href="/tag/deployment/" rel="tag">deployment</a>, <a href="/tag/gpu/" rel="tag">GPU</a>, <a href="/tag/machine-learning/" rel="tag">machine learning</a>, <a href="/tag/optimization/" rel="tag">optimization</a>, <a href="/tag/research/" rel="tag">research</a>, <a href="/tag/training/" rel="tag">training</a></span>
</div>	</footer><!-- .entry-footer -->

				
</article><!-- #post-59 -->

	<nav class="navigation post-navigation" role="navigation" aria-label="Posts">
		<h2 class="screen-reader-text">Post navigation</h2>
		<div class="nav-links"><div class="nav-previous"><a href="/achieve-b1-german-in-6-months/" rel="prev"><p class="meta-nav"><svg class="svg-icon" width="24" height="24" aria-hidden="true" role="img" focusable="false" viewbox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M20 13v-2H8l4-4-1-2-7 7 7 7 1-2-4-4z" fill="currentColor"></path></svg>Previous post</p>
<p class="post-title">Achieve B1 German in 6 months</p></a></div></div>
	</nav>			</main><!-- #main -->
		</div>
<!-- #primary -->
	</div>
<!-- #content -->

	
	<aside class="widget-area">
		
		<section id="recent-posts-2" class="widget widget_recent_entries">
		<h2 class="widget-title">Recent Posts</h2>
<nav role="navigation" aria-label="Recent Posts">
		<ul>
											<li>
					<a href="/cut-gpu-from-machine-learning/" aria-current="page">Cut GPU from machine learning</a>
									</li>
											<li>
					<a href="/achieve-b1-german-in-6-months/">Achieve B1 German in 6 months</a>
									</li>
					</ul>

		</nav></section>	</aside><!-- .widget-area -->


	<footer id="colophon" class="site-footer" role="contentinfo">

					<nav aria-label="Secondary menu" class="footer-navigation">
				<ul class="footer-navigation-wrapper">
					<li id="menu-item-17" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-17"><a href="http://www.github.com/MetcalfeTom"><svg class="svg-icon" width="24" height="24" aria-hidden="true" role="img" focusable="false" viewbox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg"><path d="M12,2C6.477,2,2,6.477,2,12c0,4.419,2.865,8.166,6.839,9.489c0.5,0.09,0.682-0.218,0.682-0.484 c0-0.236-0.009-0.866-0.014-1.699c-2.782,0.602-3.369-1.34-3.369-1.34c-0.455-1.157-1.11-1.465-1.11-1.465 c-0.909-0.62,0.069-0.608,0.069-0.608c1.004,0.071,1.532,1.03,1.532,1.03c0.891,1.529,2.341,1.089,2.91,0.833 c0.091-0.647,0.349-1.086,0.635-1.337c-2.22-0.251-4.555-1.111-4.555-4.943c0-1.091,0.39-1.984,1.03-2.682 C6.546,8.54,6.202,7.524,6.746,6.148c0,0,0.84-0.269,2.75,1.025C10.295,6.95,11.15,6.84,12,6.836 c0.85,0.004,1.705,0.114,2.504,0.336c1.909-1.294,2.748-1.025,2.748-1.025c0.546,1.376,0.202,2.394,0.1,2.646 c0.64,0.699,1.026,1.591,1.026,2.682c0,3.841-2.337,4.687-4.565,4.935c0.359,0.307,0.679,0.917,0.679,1.852 c0,1.335-0.012,2.415-0.012,2.741c0,0.269,0.18,0.579,0.688,0.481C19.138,20.161,22,16.416,22,12C22,6.477,17.523,2,12,2z"></path></svg><span class="screen-reader-text">Github</span></a></li>
<li id="menu-item-18" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-18"><a href="https://www.linkedin.com/in/thomas-metcalfe/"><svg class="svg-icon" width="24" height="24" aria-hidden="true" role="img" focusable="false" viewbox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg"><path d="M19.7,3H4.3C3.582,3,3,3.582,3,4.3v15.4C3,20.418,3.582,21,4.3,21h15.4c0.718,0,1.3-0.582,1.3-1.3V4.3 C21,3.582,20.418,3,19.7,3z M8.339,18.338H5.667v-8.59h2.672V18.338z M7.004,8.574c-0.857,0-1.549-0.694-1.549-1.548 c0-0.855,0.691-1.548,1.549-1.548c0.854,0,1.547,0.694,1.547,1.548C8.551,7.881,7.858,8.574,7.004,8.574z M18.339,18.338h-2.669 v-4.177c0-0.996-0.017-2.278-1.387-2.278c-1.389,0-1.601,1.086-1.601,2.206v4.249h-2.667v-8.59h2.559v1.174h0.037 c0.356-0.675,1.227-1.387,2.526-1.387c2.703,0,3.203,1.779,3.203,4.092V18.338z"></path></svg><span class="screen-reader-text">LinkedIn</span></a></li>
<li id="menu-item-19" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-19"><a href="mailto:thomasjosephmetcalfe@gmail.com"><svg class="svg-icon" width="24" height="24" aria-hidden="true" role="img" focusable="false" viewbox="0 0 24 24" version="1.1" xmlns="http://www.w3.org/2000/svg"><path d="M20,4H4C2.895,4,2,4.895,2,6v12c0,1.105,0.895,2,2,2h16c1.105,0,2-0.895,2-2V6C22,4.895,21.105,4,20,4z M20,8.236l-8,4.882 L4,8.236V6h16V8.236z"></path></svg><span class="screen-reader-text">Email</span></a></li>
				</ul>
<!-- .footer-navigation-wrapper -->
			</nav><!-- .footer-navigation -->
				<div class="site-info">
			<div class="site-name">
																						<a href="/">Thomas Metcalfe</a>
																		</div>
<!-- .site-name -->
		</div>
<!-- .site-info -->
	</footer><!-- #colophon -->

</div>
<!-- #page -->

<script>document.body.classList.remove("no-js");</script>	<script>
	if ( -1 !== navigator.userAgent.indexOf( 'MSIE' ) || -1 !== navigator.appVersion.indexOf( 'Trident/' ) ) {
		document.body.classList.add( 'is-IE' );
	}
	</script>
	<script type="text/javascript">!function(t,e){"use strict";function n(){if(!a){a=!0;for(var t=0;t<d.length;t++)d[t].fn.call(window,d[t].ctx);d=[]}}function o(){"complete"===document.readyState&&n()}t=t||"docReady",e=e||window;var d=[],a=!1,c=!1;e[t]=function(t,e){return a?void setTimeout(function(){t(e)},1):(d.push({fn:t,ctx:e}),void("complete"===document.readyState||!document.attachEvent&&"interactive"===document.readyState?setTimeout(n,1):c||(document.addEventListener?(document.addEventListener("DOMContentLoaded",n,!1),window.addEventListener("load",n,!1)):(document.attachEvent("onreadystatechange",o),window.attachEvent("onload",n)),c=!0)))}}("wpBruiserDocReady",window);
			(function(){var wpbrLoader = (function(){var g=document,b=g.createElement('script'),c=g.scripts[0];b.async=1;b.src='/?gdbc-client=3.1.43-'+(new Date()).getTime();c.parentNode.insertBefore(b,c);});wpBruiserDocReady(wpbrLoader);window.onunload=function(){};window.addEventListener('pageshow',function(event){if(event.persisted){(typeof window.WPBruiserClient==='undefined')?wpbrLoader():window.WPBruiserClient.requestTokens();}},false);})();
</script><script id="twenty-twenty-one-ie11-polyfills-js-after">
( Element.prototype.matches && Element.prototype.closest && window.NodeList && NodeList.prototype.forEach ) || document.write( '<script src="/wp-content/themes/twentytwentyone/assets/js/polyfills.js?ver=1.3"><\/scr' + 'ipt>' );
</script>
<script src="/wp-content/themes/twentytwentyone/assets/js/primary-navigation.js?ver=1.3" id="twenty-twenty-one-primary-navigation-script-js"></script>
<script src="/wp-content/themes/twentytwentyone/assets/js/responsive-embeds.js?ver=1.3" id="twenty-twenty-one-responsive-embeds-script-js"></script>
<script src="/wp-includes/js/wp-embed.min.js?ver=5.7.1" id="wp-embed-js"></script>
	<script>
	/(trident|msie)/i.test(navigator.userAgent)&&document.getElementById&&window.addEventListener&&window.addEventListener("hashchange",(function(){var t,e=location.hash.substring(1);/^[A-z0-9_-]+$/.test(e)&&(t=document.getElementById(e))&&(/^(?:a|select|input|button|textarea)$/i.test(t.tagName)||(t.tabIndex=-1),t.focus())}),!1);
	</script>
	
</body>
</html>
